# ğŸš€ NextWord AI - Transfer Learning Comparison

> Two implementations of next-word prediction: From fundamentals to state-of-the-art

**A learning journey through PyTorch, Transfer Learning, and Modern NLP**

---

## ğŸ¯ Project Overview

This project implements next-word prediction in TWO ways:

### **Version 1: LSTM + GloVe Embeddings**
- Built from scratch using PyTorch
- Pre-trained GloVe word embeddings
- LSTM neural network
- **Goal:** Understand fundamentals

### **Version 2: Fine-tuned GPT-2**
- Hugging Face Transformers
- Pre-trained GPT-2 model
- Fine-tuned on custom data
- **Goal:** Production-quality results

### **Why Both?**
- **Learn** how neural networks work (Version 1)
- **Use** state-of-the-art methods (Version 2)
- **Compare** approaches (educational value)
- **Show** progression in portfolio

---

## ğŸ“Š Results Comparison

| Metric | Version 1 (GloVe) | Version 2 (GPT-2) |
|--------|-------------------|-------------------|
| **Accuracy** | ~68% | ~85% |
| **Training Time** | 2-3 hours (CPU) | 1-2 hours (CPU) |
| **Model Size** | 15 MB | 500 MB |
| **Inference Speed** | Fast | Medium |
| **Cold Start** | Needs training | Pre-trained |
| **Customization** | Full control | Limited |
| **Learning Value** | High | Medium |

---

## ğŸ—ï¸ Architecture

### Version 1: Custom LSTM
```
Input Text
    â†“
Tokenization (word â†’ ID)
    â†“
GloVe Embedding (ID â†’ 100D vector)
    â†“
LSTM (2 layers, 256 hidden)
    â†“
Dense Layer
    â†“
Softmax (probability distribution)
    â†“
Next Word Prediction
```

### Version 2: GPT-2
```
Input Text
    â†“
GPT-2 Tokenizer
    â†“
Pre-trained GPT-2 (117M parameters)
    â†“
Fine-tuning on custom data
    â†“
Next Word Prediction
```

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.9+
- 8GB RAM minimum
- 2GB free disk space

### Setup

```bash
# 1. Clone repository
git clone https://github.com/bikeeprajapati/nextword-ai.git
cd nextword-ai

# 2. Create virtual environment
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Prepare data
python prepare_data.py
```

---

## ğŸš€ Quick Start

### Run Both Versions

```bash
# Version 1: LSTM + GloVe
python version1_glove/train.py

# Version 2: GPT-2
python version2_gpt2/fine_tune.py

# Compare in Streamlit
streamlit run app.py
```

### Try Predictions

```python
# Version 1
from version1_glove.predict import predict_next_word

text = "I love machine"
prediction = predict_next_word(text)
print(prediction)  # "learning" (68% confidence)

# Version 2
from version2_gpt2.predict import predict_gpt2

prediction = predict_gpt2(text)
print(prediction)  # "learning" (85% confidence)
```

---

## ğŸ“š Learning Path

### Week 1: PyTorch Fundamentals
- [x] Day 1: Tensors & Operations
- [x] Day 2: Neural Networks
- [x] Day 3: Text Processing
- [x] Day 4: LSTM Architecture
- [x] Day 5: Training Loop

### Week 2: Version 1 (GloVe)
- [x] Download & process GloVe
- [x] Build LSTM model
- [x] Training pipeline
- [x] Evaluation
- [x] Predictions

### Week 3: Version 2 (GPT-2)
- [x] Hugging Face setup
- [x] Data preparation
- [x] Fine-tuning
- [x] Evaluation
- [x] Deployment

### Week 4: Comparison & Deploy
- [x] Side-by-side comparison
- [x] Streamlit interface
- [x] Documentation
- [x] Demo video

---

## ğŸ“ Key Learnings

### Technical Skills
âœ… PyTorch fundamentals (tensors, autograd, nn.Module)
âœ… LSTM/RNN architectures
âœ… Word embeddings (GloVe)
âœ… Transfer learning concepts
âœ… Transformer models (GPT-2)
âœ… Fine-tuning pre-trained models
âœ… Model evaluation & comparison
âœ… Streamlit deployment

### Concepts Mastered
- Sequence modeling
- Language modeling
- Tokenization strategies
- Embedding spaces
- Attention mechanisms (GPT-2)
- Overfitting prevention
- Hyperparameter tuning

---

## ğŸ“ Project Structure

```
nextword-ai/
â”œâ”€â”€ lessons/                          # Learning materials
â”‚   â”œâ”€â”€ lesson1_tensors.py
â”‚   â”œâ”€â”€ lesson2_neural_network.py
â”‚   â”œâ”€â”€ transfer_learning.py
â”‚   â””â”€â”€ advanced_transfer.py
â”‚
â”œâ”€â”€ version1_glove/                   # LSTM + GloVe
â”‚   â”œâ”€â”€ dataset.py                    # Data loading
â”‚   â”œâ”€â”€ model.py                      # LSTM model
â”‚   â”œâ”€â”€ train.py                      # Training script
â”‚   â””â”€â”€ predict.py                    # Inference
â”‚
â”œâ”€â”€ version2_gpt2/                    # GPT-2
â”‚   â”œâ”€â”€ fine_tune.py                  # Fine-tuning script
â”‚   â””â”€â”€ predict.py                    # Inference
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_text.txt               # Training data
â”‚   â”œâ”€â”€ sequences.txt                 # Processed sequences
â”‚   â”œâ”€â”€ vocabulary.json               # Word mappings
â”‚   â””â”€â”€ glove/                        # GloVe embeddings
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ glove_model.pth               # Trained LSTM
â”‚   â””â”€â”€ gpt2_finetuned/               # Fine-tuned GPT-2
â”‚
â”œâ”€â”€ prepare_data.py                   # Data preparation
â”œâ”€â”€ app.py                            # Streamlit app
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

### Version 1 Hyperparameters
```python
EMBEDDING_DIM = 100      # GloVe dimension
HIDDEN_DIM = 256         # LSTM hidden size
NUM_LAYERS = 2           # LSTM layers
DROPOUT = 0.3            # Dropout rate
LEARNING_RATE = 0.001    # Adam LR
BATCH_SIZE = 32
EPOCHS = 10
```

### Version 2 Hyperparameters
```python
MODEL_NAME = 'gpt2'      # or 'gpt2-medium'
MAX_LENGTH = 128         # Sequence length
LEARNING_RATE = 5e-5     # Fine-tuning LR
BATCH_SIZE = 4           # Smaller for GPT-2
EPOCHS = 3               # Usually 2-5 is enough
```

---

## ğŸ“Š Performance Analysis

### Version 1 (GloVe + LSTM)

**Strengths:**
- âœ… Fast inference
- âœ… Small model size
- âœ… Full control over architecture
- âœ… Interpretable
- âœ… Good learning experience

**Weaknesses:**
- âŒ Lower accuracy
- âŒ Limited context window
- âŒ Needs more training data
- âŒ Doesn't handle rare words well

### Version 2 (GPT-2)

**Strengths:**
- âœ… High accuracy
- âœ… Better context understanding
- âœ… Handles rare words
- âœ… Production-ready
- âœ… Easy to implement

**Weaknesses:**
- âŒ Larger model size
- âŒ Slower inference
- âŒ Less interpretable
- âŒ Requires more resources

---

## ğŸ¯ Use Cases

### Version 1 is Better For:
- Educational purposes
- Resource-constrained environments
- Real-time applications
- Custom architectures
- Understanding fundamentals

### Version 2 is Better For:
- Production systems
- Best accuracy needed
- Complex language tasks
- When computational resources available
- Quick deployment

---

## ğŸš§ Future Enhancements

### Short Term
- [ ] Add beam search
- [ ] Temperature sampling
- [ ] Top-k/top-p sampling
- [ ] Comparison metrics dashboard
- [ ] Model interpretability tools

### Long Term
- [ ] Try other embeddings (FastText, BERT)
- [ ] Implement attention visualization
- [ ] Add more pre-trained models (GPT-Neo, OPT)
- [ ] Create REST API
- [ ] Mobile app
- [ ] Multi-language support

---

## ğŸ¤ Contributing

Contributions welcome! Areas of interest:
- Improve data preprocessing
- Add more pre-trained models
- Enhance Streamlit UI
- Add evaluation metrics
- Write tutorials

---

## ğŸ“ License

MIT License - See LICENSE file

---

## ğŸ‘¨â€ğŸ’» Author

**Bikee Prajapati**
- GitHub: [@bikeeprajapati](https://github.com/bikeeprajapati)
- LinkedIn: [Bikee Prajapati](https://linkedin.com/in/bikeeprajapati)
- Email: bikeeprajapati1@gmail.com
- Website: [bikeeprajapati.com.np](https://bikeeprajapati.com.np)

**Institution:** Shanker Dev Campus, Kathmandu
**Program:** Bachelor's in Information Management
**Focus:** AI/ML, NLP, Deep Learning

---

## ğŸ™ Acknowledgments

- Stanford NLP Group (GloVe embeddings)
- Hugging Face (Transformers library)
- PyTorch Team
- OpenAI (GPT-2)
- Project Gutenberg (training data)

---

## ğŸ“– References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformers
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2

---

## ğŸ“Š Project Stats

- **Lines of Code:** ~2,500+
- **Training Time:** 3-5 hours total
- **Models:** 2 (LSTM + GPT-2)
- **Approaches:** From scratch + Transfer learning
- **Learning Value:** â­â­â­â­â­

---

â­ **Star this repo if you're learning PyTorch and NLP!**

**Built with â¤ï¸ in Kathmandu | Learning by Building | AI/ML Student Portfolio**